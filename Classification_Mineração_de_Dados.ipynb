{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification - Mineração de Dados.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamteles/Classification_Data-Mining/blob/main/Classification_Minera%C3%A7%C3%A3o_de_Dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMDgRm5km_rU"
      },
      "source": [
        "# !pip install pycaret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehL5ZhWZ_d-s"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_profiling\n",
        "from sklearn import model_selection\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "from pycaret.classification import setup\n",
        "from pycaret.classification import compare_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HJV5gGcrInf"
      },
      "source": [
        "## Lendo o Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtJy8jD9_qqa"
      },
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00571/hcvdat0.csv\"\n",
        "df = pd.read_csv(url, delimiter=',', index_col=0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWQxR8U8rOYL"
      },
      "source": [
        "## Transformando Atributos categóricos para Binário"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLH5nMZrFbbs"
      },
      "source": [
        "categories = df.Category.unique()\n",
        "categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvWWPF0AD7k-"
      },
      "source": [
        "df.Category.replace(categories, [0,0,1,1,1], inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGHsoiGEGNjA"
      },
      "source": [
        "# Transformando de um dado categórico para binário\n",
        "df['Sex'] = df['Sex'].map({'m': 0, 'f': 1})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08qgQKtMrd7e"
      },
      "source": [
        "## Removendo instâncias com dados nulos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUvU5vxnHuZo"
      },
      "source": [
        "# Checando se exixte dados nulos\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIfI0t_tIank"
      },
      "source": [
        "# Deletando os dados que possuem valores nulos\n",
        "df = df.dropna()\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDZEF8-yrnHh"
      },
      "source": [
        "## Análise estatística dos Atributos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2_S4yHsnmDx"
      },
      "source": [
        "# Analisando a média/desvio padrão\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FNbVzzis874"
      },
      "source": [
        "## Funções para limitar anomalias e normalizar os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdonQJhXoyEd"
      },
      "source": [
        "# função para limitar os dados\n",
        "def limitar_anomalias(data, anomalia_cols):\n",
        "  for col in anomalia_cols:\n",
        "    q25, q75 = np.percentile(data[col], 25), np.percentile(data[col], 75)\n",
        "\n",
        "    iiq = q75 - q25\n",
        "\n",
        "    print(f'Percentis coluna {col}: 25% = {q25:.3f}, 75% = {q75:.3f}, IIQ = {iiq:.3f}')\n",
        "\n",
        "    corte = iiq * 1.5\n",
        "\n",
        "    inferior, superior = q25 - corte, q75 + corte\n",
        "\n",
        "    anomalias = [x for x in data[col] if x < inferior or x > superior]\n",
        "\n",
        "    print(f'Outliers Identificados na coluna {col}: {len(anomalias)}')\n",
        "    print()\n",
        "\n",
        "    data[col] = np.clip(data[col], inferior, superior)\n",
        "  \n",
        "  return data\n",
        "\n",
        "# função para normalização\n",
        "def normalizar(data, normalizar_cols):\n",
        "  for col in normalizar_cols:\n",
        "    data[col] = (data[col] - data[col].min()) / (data[col].max() - data[col].min())\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrISUFrJtSzw"
      },
      "source": [
        "## Limitando os dados para remover anomalias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3uQyqSzqWuC"
      },
      "source": [
        "# Setando os atributos para terem anomalias limitadas\n",
        "anomalia_cols = ['ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']\n",
        "df = limitar_anomalias(df.copy(), anomalia_cols)\n",
        "\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6vWEDPDtasI"
      },
      "source": [
        "## Normalizando dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUnMA43_vXVr"
      },
      "source": [
        "# setando os atributos a serem normalizados\n",
        "normalizar_cols = ['ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']\n",
        "\n",
        "df = normalizar(df.copy(), anomalia_cols)\n",
        "\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m7XyR9OtmBo"
      },
      "source": [
        "## Checando dados duplicados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbmL4uquTH5c"
      },
      "source": [
        "# checando se a dados duplicados\n",
        "duplicados = df.duplicated()\n",
        "print(duplicados.any())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhfTTcuvuC0f"
      },
      "source": [
        "## Checando o balanceamento das classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2BFwZ5Mtywh"
      },
      "source": [
        "df.Category.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf6l39httpU0"
      },
      "source": [
        "## Teste para ver se depois do OverSample teria dados duplicados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYIeaj7edI7P"
      },
      "source": [
        "# realizando o balanceamento das classes utilizando o oversample\n",
        "mascara = df.Category == 0\n",
        "df_0 = df[mascara]\n",
        "df_1 = df[~mascara]\n",
        "df_oversample = resample(df_1, n_samples=len(df_0), random_state=13)\n",
        "df2 = pd.concat([df_0, df_oversample])\n",
        "df2.Category.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TMZe5yl7bJg"
      },
      "source": [
        "# checando novamente se há dados duplicados após o balanceamento, e sim foi encontrado dados duplicados o que poderia dar algo errado mais na frente \n",
        "# se por acaso ele jogase algum dado na base de treino que fosse igual ou muito parecido com o da outra base, podendo viciar o modelo.Y\n",
        "duplicados = df2.duplicated()\n",
        "print(duplicados.any())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr_Wr8Slt6Ig"
      },
      "source": [
        "## Modelagem e Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR7qJG4giYAK"
      },
      "source": [
        "# inicializando o Pycaret\n",
        "clf = setup(data=df, target='Category', silent=True, verbose=False, log_experiment=True, session_id=13)\n",
        "# pegando os 5 melhores modelos\n",
        "best = compare_models(n_select=5)\n",
        "for model in best:\n",
        "  print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZgpD38j1yWT"
      },
      "source": [
        "# instanciando os modelos \n",
        "model1 = [AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
        "                   n_estimators=50, random_state=13), 'AdaBoost']\n",
        "model2 = [RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "                       criterion='gini', max_depth=None, max_features='auto',\n",
        "                       max_leaf_nodes=None, max_samples=None,\n",
        "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                       min_samples_leaf=1, min_samples_split=2,\n",
        "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "                       n_jobs=-1, oob_score=False, random_state=13, verbose=0,\n",
        "                       warm_start=False),'Random Forest']\n",
        "# colocando na lista para usar na validação cruzada\n",
        "models = [model1, model2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfLcEcEe4A9z"
      },
      "source": [
        "y = df['Category'].values\n",
        "X = df.drop(columns='Category').values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZsSn8D797PV"
      },
      "source": [
        "# Função para fazer o oversampling nos dados de treino\n",
        "def overSampling(X_train, y_train):\n",
        "  mascara = y_train == 0\n",
        "  y_0 = y_train[mascara]\n",
        "  y_1 = y_train[~mascara]\n",
        "  X_0 = X_train[mascara]\n",
        "  X_1 = X_train[~mascara]\n",
        "  y_oversample = resample(y_1, n_samples=len(y_0), random_state=1)\n",
        "  X_oversample = resample(X_1, n_samples=len(X_0), random_state=1)\n",
        "  y_train = np.concatenate((y_0, y_oversample))\n",
        "  X_train = np.concatenate((X_0, X_oversample))\n",
        "  # print(f'Class 0 / 1 quant: {y_train[y_train == 0].size} / {y_train[y_train == 1].size}')\n",
        "\n",
        "  return X_train, y_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYy_keR23EAG"
      },
      "source": [
        "# Fazendo o treinamento e teste dos modelos com validação cruzada\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "for mod in models:\n",
        "    acc = []\n",
        "    f1 = []\n",
        "    precision = []\n",
        "\n",
        "    for train_indexs, test_indexs in skf.split(X, y):\n",
        "        X_train, y_train = overSampling(X[train_indexs,:], y[train_indexs])\n",
        "        X_test, y_test = X[test_indexs,:], y[test_indexs]\n",
        "\n",
        "        model = mod[0]\n",
        "        model.fit(X_train, y_train)\n",
        "        y_predicted = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_predicted)\n",
        "        f1_scr = f1_score(y_test, y_predicted)\n",
        "        prec_score = precision_score(y_test, y_predicted)\n",
        "        acc.append(accuracy)\n",
        "        f1.append(f1_scr)\n",
        "        precision.append(prec_score)\n",
        "\n",
        "    print(f\"{mod[1]}, Accuracy:\\t{np.mean(acc):.4f} +/- {np.std(acc):.4f}\")\n",
        "    print(f\"{mod[1]}, F1 Score:\\t{np.mean(f1):.4f} +/- {np.std(f1):.4f}\")\n",
        "    print(f\"{mod[1]}, Precision:\\t{np.mean(precision):.4f} +/- {np.std(precision):.4f}\")\n",
        "    print(\"-\"*60)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}